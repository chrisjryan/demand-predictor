{
 "metadata": {
  "name": "",
  "signature": "sha256:a3e6302c106d19bfe0db50b4e3fd6aace38de61bddb90210559dbd734c21c801"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\n",
      "import datetime\n",
      "from collections import namedtuple\n",
      "import numpy\n",
      "import os\n",
      "\n",
      "import dataprep\n",
      "\n",
      "Stats = namedtuple('Stats', ['mean','var'])\n",
      "\n",
      "\n",
      "def exp_downweight_avg(dates_counts, s = 0.97):\n",
      "    \"\"\"\n",
      "    Average the timestamp counts for a particular hour of a particular \n",
      "    weekday. Older data are downweighted exponentially, so newer data can \n",
      "    have more influence on the average. At the moment the parameter for this \n",
      "    scheme is simply read in as an optional argument.\n",
      "    \"\"\"\n",
      "\n",
      "    # dividing by the sum of the weights will normalize our statistics.\n",
      "    # (note: in the limit of lots of data this will = 1-s. But we can't \n",
      "    # guarantee that yet.)\n",
      "    date_most_recent = max(zip(*dates_counts)[0])\n",
      "    weight_list = []\n",
      "    norm_const = 0\n",
      "    running_mean = 0    \n",
      "    for date, count in dates_counts:\n",
      "        weight = s**((date_most_recent - date).days/7)\n",
      "        running_mean += count*weight\n",
      "        norm_const += weight\n",
      "        weight_list.append(weight)\n",
      "    m = running_mean/norm_const\n",
      "\n",
      "    # TODO: this is a biased weighted sample variance, for now, which is close.\n",
      "    # eventually take time to consider the unbiased one.\n",
      "    counts = zip(*dates_counts)[1]\n",
      "    running_var = 0\n",
      "    for weight, count in zip(weight_list, counts):\n",
      "        running_var += (count-m)**2.0*weight\n",
      "    v = running_var/norm_const\n",
      "        \n",
      "    return Stats(mean = m, var = v)\n",
      "\n",
      "\n",
      "def weekday_hour_grouping(hours, usage, filter_holidays = False):\n",
      "    \"\"\"\n",
      "    This function groups the 1d arrays of binned timestamps count data with \n",
      "    other data on the same weekday and hour, but on different dates.\n",
      "    The output, 'weekhour_agg', is a 2d list with each element correpsonding \n",
      "    to an hour of a week day (hence the 2 dimensions). Each of these elements \n",
      "    contains a list of 2-tuples, where 2-tuple contains this hour window for \n",
      "    some specific date and the number of timestamps binned in this window on \n",
      "    this date. Maintaining the data at this point helps keep the age of the \n",
      "    data before averaging, so this can be weighted properly using, e.g., the \n",
      "    exponential downweighting method.\n",
      "    \"\"\"\n",
      "    \n",
      "    # filter out \"special days\"\n",
      "#     if (filter_holidays):\n",
      "#         hours, usage = filter_special_days(hours, usage)\n",
      "    \n",
      "    # for the dataset, aggregate data on the same hour of the same weekday:\n",
      "    weekhour_agg = [[[] for _ in range(24)] for _ in range (7)]\n",
      "\n",
      "    # NOTE: if the hours array is ordered, you could do this without reaching into each datetime object\n",
      "    for h, u in zip(hours, usage):\n",
      "        wd = h.weekday()\n",
      "        hr = h.timetuple().tm_hour\n",
      "        weekhour_agg[wd][hr].append((h,u))\n",
      "\n",
      "    return weekhour_agg\n",
      "\n",
      "\n",
      "def average_hour(weekhour, method='average-plain'):\n",
      "    \"\"\"\n",
      "    Average the timestamp counts for a particular hour of a particular \n",
      "    weekday. At the moment, counts may be averaged normally or older data \n",
      "    can be exponentially downweighted in favor of newer data. (See docstring \n",
      "    for exp_downweight_avg().)\n",
      "    \"\"\"\n",
      "\n",
      "    if method == 'average-plain':\n",
      "        usagecounts = zip(*weekhour)[1]\n",
      "        return Stats(mean = numpy.mean(usagecounts), \\\n",
      "                     var  = numpy.var(usagecounts, ddof=1))\n",
      "\n",
      "    if method == 'exp-downweight':\n",
      "        return exp_downweight_avg(weekhour)\n",
      "        \n",
      "        \n",
      "\n",
      "def average_all_hours(weekhour_agg, method='average-plain'):\n",
      "    \"\"\"\n",
      "    Compute the average and variance, using the method specified as an \n",
      "    argument, for each hour on each weekday.\n",
      "    \"\"\"\n",
      "    \n",
      "    hourly_usage_stats = [[Stats(mean=None, var=None) for _ in range(24)] for _ in range (7)]\n",
      "    for wd in range(7):\n",
      "        for hr in range(24):\n",
      "            hourly_usage_stats[wd][hr] = average_hour(weekhour_agg[wd][hr], method)\n",
      "\n",
      "    return hourly_usage_stats\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    \"\"\"\n",
      "    Load the JSON file containing timestamp data and compute average usage \n",
      "    statistics for each hour in a week. \n",
      "    \"\"\"\n",
      "    # A list of datetime obects that specify atypical days (holidays, etc) \n",
      "    # (to be updated manually):\n",
      "#     special_dates = []\n",
      "\n",
      "    # load the JSON file (throw sensible error if it doesn't exist):\n",
      "    # (each member of the hours, usage lists corresponds to a unique hour on a unique date.)\n",
      "    datafile = \"hourly_demand_prediction_challenge.json\"\n",
      "    if os.path.isfile(datafile):\n",
      "        hours, usage = dataprep.prepare(datafile)\n",
      "\n",
      "        # perform the hourly grouping & averaging:\n",
      "        # (add here a method to group MTWR data together, if needed)\n",
      "        weekhour_agg = weekday_hour_grouping(hours, usage)\n",
      "        method='average-plain'\n",
      "        hourly_usage_stats = average_all_hours(weekhour_agg, method)\n",
      "    else:\n",
      "        print 'Error: No data file found.'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}